{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-11T08:31:06.595062Z",
     "start_time": "2024-09-11T08:28:20.728095Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from keras.api.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Paths for training, testing, and validation images\n",
    "train_path = 'nude_sexy_safe_v1_x320/training'\n",
    "test_path = 'nude_sexy_safe_v1_x320/testing'\n",
    "validation_path = 'nude_sexy_safe_v1_x320/validation'\n",
    "\n",
    "# Number of classes (update this based on your dataset)\n",
    "num_classes = 3\n",
    "\n",
    "# Preprocessing pipeline for training data\n",
    "preprocessing_pipeline_train = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1./255),\n",
    "    tf.keras.layers.Resizing(height=64, width=64),\n",
    "    tf.keras.layers.RandomCrop(height=64, width=64),\n",
    "    tf.keras.layers.RandomRotation(factor=0.2, fill_mode=\"reflect\", interpolation=\"bilinear\"),\n",
    "    tf.keras.layers.RandomZoom(height_factor=(0.2, 0.3), fill_mode=\"reflect\", interpolation=\"bilinear\"),\n",
    "    tf.keras.layers.RandomBrightness(factor=0.1)\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for testing and validation data\n",
    "preprocessing_pipeline_test = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(scale=1./255),\n",
    "])\n",
    "\n",
    "# Create a subset of the train dataset for demo purposes (e.g., limit to 1000 images)\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_path,\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64),\n",
    "    shuffle=True,     # Shuffle to get a random subset\n",
    "    seed=42           # Set seed for reproducibility\n",
    ")\n",
    "\n",
    "# Limit the number of batches to a smaller subset (for example, 50 batches * 32 images = 1600 images)\n",
    "train_dataset = train_dataset.take(50)\n",
    "train_dataset = train_dataset.map(lambda x, y: (preprocessing_pipeline_train(x), tf.one_hot(y, num_classes)))\n",
    "\n",
    "# Create the validation dataset from the 'validation' directory\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_path,\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64)\n",
    ")\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (preprocessing_pipeline_test(x), tf.one_hot(y, num_classes)))\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_path,\n",
    "    label_mode='int',\n",
    "    batch_size=32,\n",
    "    image_size=(64, 64)\n",
    ")\n",
    "test_dataset = test_dataset.map(lambda x, y: (preprocessing_pipeline_test(x), tf.one_hot(y, num_classes)))\n",
    "\n",
    "# CNN model\n",
    "cnn = tf.keras.models.Sequential()\n",
    "cnn.add(tf.keras.Input(shape=(64, 64, 3)))\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "cnn.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the subset of training data and validate with the validation dataset\n",
    "cnn.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,  # Use the separate validation dataset\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = cnn.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-11 14:28:21.706201: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-11 14:28:21.909278: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-11 14:28:22.702071: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-11 14:28:24.820422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 664792 files belonging to 3 classes.\n",
      "Found 10306 files belonging to 3 classes.\n",
      "Found 10049 files belonging to 3 classes.\n",
      "Epoch 1/5\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 376ms/step - accuracy: 0.5693 - loss: 2.1921 - val_accuracy: 0.3881 - val_loss: 1.0868\n",
      "Epoch 2/5\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 364ms/step - accuracy: 0.6326 - loss: 0.8451 - val_accuracy: 0.3881 - val_loss: 1.0923\n",
      "Epoch 3/5\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 377ms/step - accuracy: 0.6385 - loss: 0.8658 - val_accuracy: 0.3881 - val_loss: 1.1004\n",
      "Epoch 4/5\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 357ms/step - accuracy: 0.6530 - loss: 0.8150 - val_accuracy: 0.3881 - val_loss: 1.0966\n",
      "Epoch 5/5\n",
      "\u001B[1m50/50\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 391ms/step - accuracy: 0.6395 - loss: 0.8363 - val_accuracy: 0.3881 - val_loss: 1.0933\n",
      "\u001B[1m315/315\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 54ms/step - accuracy: 0.3891 - loss: 1.0769\n",
      "Test Accuracy: 0.3859\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b494c36569b5dad3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
